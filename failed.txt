++ id -u
+ myuid=0
++ id -g
+ mygid=0
+ set +e
++ getent passwd 0
+ uidentry=root:x:0:0:root:/root:/bin/ash
+ set -e
+ '[' -z root:x:0:0:root:/root:/bin/ash ']'
+ SPARK_K8S_CMD=driver
+ case "$SPARK_K8S_CMD" in
+ shift 1
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -n '' ']'
+ PYSPARK_ARGS=
+ '[' -n '' ']'
+ R_ARGS=
+ '[' -n '' ']'
+ '[' '' == 2 ']'
+ '[' '' == 3 ']'
+ case "$SPARK_K8S_CMD" in
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /sbin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.233.69.240 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class com.github.yannbolliger.g5.parallel.sgd.spark.ParallelSGDApp spark-internal 100 1000
19/04/10 07:51:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/04/10 07:51:21 INFO SparkContext: Running Spark version 2.4.1
19/04/10 07:51:21 INFO SparkContext: Submitted application: g5-parallel-sgd-spark
19/04/10 07:51:21 INFO SecurityManager: Changing view acls to: root
19/04/10 07:51:21 INFO SecurityManager: Changing modify acls to: root
19/04/10 07:51:21 INFO SecurityManager: Changing view acls groups to: 
19/04/10 07:51:21 INFO SecurityManager: Changing modify acls groups to: 
19/04/10 07:51:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/04/10 07:51:21 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
19/04/10 07:51:21 INFO SparkEnv: Registering MapOutputTracker
19/04/10 07:51:21 INFO SparkEnv: Registering BlockManagerMaster
19/04/10 07:51:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/10 07:51:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/10 07:51:21 INFO DiskBlockManager: Created local directory at /var/data/spark-5c3481da-5828-4124-b63b-c0902cb43d14/blockmgr-1dc37d97-475c-4a5e-b474-0fab6e20da96
19/04/10 07:51:21 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
19/04/10 07:51:21 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/10 07:51:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/10 07:51:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-1554882676220-driver-svc.cs449g5.svc:4040
19/04/10 07:51:21 INFO SparkContext: Added JAR file:///opt/code.jar at spark://spark-1554882676220-driver-svc.cs449g5.svc:7078/jars/code.jar with timestamp 1554882681745
19/04/10 07:51:26 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/04/10 07:51:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
19/04/10 07:51:26 INFO NettyBlockTransferService: Server created on spark-1554882676220-driver-svc.cs449g5.svc:7079
19/04/10 07:51:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/10 07:51:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-1554882676220-driver-svc.cs449g5.svc, 7079, None)
19/04/10 07:51:26 INFO BlockManagerMasterEndpoint: Registering block manager spark-1554882676220-driver-svc.cs449g5.svc:7079 with 2004.6 MB RAM, BlockManagerId(driver, spark-1554882676220-driver-svc.cs449g5.svc, 7079, None)
19/04/10 07:51:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-1554882676220-driver-svc.cs449g5.svc, 7079, None)
19/04/10 07:51:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-1554882676220-driver-svc.cs449g5.svc, 7079, None)
19/04/10 07:51:32 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/04/10 07:51:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.67.31:41326) with ID 5
19/04/10 07:51:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.88.218:52386) with ID 4
19/04/10 07:51:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.89.56:34434) with ID 3
19/04/10 07:51:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.69.61:39982) with ID 2
19/04/10 07:51:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.67.31:41441 with 2.1 GB RAM, BlockManagerId(5, 10.233.67.31, 41441, None)
19/04/10 07:51:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.78.32:34942) with ID 1
19/04/10 07:51:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.88.218:33513 with 2.1 GB RAM, BlockManagerId(4, 10.233.88.218, 33513, None)
19/04/10 07:51:37 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/04/10 07:51:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.89.56:38269 with 2.1 GB RAM, BlockManagerId(3, 10.233.89.56, 38269, None)
19/04/10 07:51:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.69.61:45149 with 2.1 GB RAM, BlockManagerId(2, 10.233.69.61, 45149, None)
19/04/10 07:51:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.94.137:49362) with ID 10
19/04/10 07:51:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.78.32:35417 with 2.1 GB RAM, BlockManagerId(1, 10.233.78.32, 35417, None)
19/04/10 07:51:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.94.137:35751 with 2.1 GB RAM, BlockManagerId(10, 10.233.94.137, 35751, None)
19/04/10 07:51:37 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.98.247:58586) with ID 6
19/04/10 07:51:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.98.247:38211 with 2.1 GB RAM, BlockManagerId(6, 10.233.98.247, 38211, None)
19/04/10 07:51:42 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.86.147:52742) with ID 12
19/04/10 07:51:42 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
19/04/10 07:51:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.86.147:33449 with 2.1 GB RAM, BlockManagerId(12, 10.233.86.147, 33449, None)
19/04/10 07:51:42 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.78.240:43554) with ID 13
19/04/10 07:51:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.78.240:42517 with 2.1 GB RAM, BlockManagerId(13, 10.233.78.240, 42517, None)
19/04/10 07:51:42 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.124.49:48538) with ID 14
19/04/10 07:51:42 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.74.189:37008) with ID 15
19/04/10 07:51:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.124.49:40293 with 2.1 GB RAM, BlockManagerId(14, 10.233.124.49, 40293, None)
19/04/10 07:51:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.74.189:43855 with 2.1 GB RAM, BlockManagerId(15, 10.233.74.189, 43855, None)
19/04/10 07:51:43 INFO BlockManagerMaster: Removal of executor 7 requested
19/04/10 07:51:43 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asked to remove non-existent executor 7
19/04/10 07:51:43 INFO BlockManagerMasterEndpoint: Trying to remove executor 7 from BlockManagerMaster.
19/04/10 07:51:44 INFO BlockManagerMasterEndpoint: Trying to remove executor 9 from BlockManagerMaster.
19/04/10 07:51:44 INFO BlockManagerMaster: Removal of executor 9 requested
19/04/10 07:51:44 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asked to remove non-existent executor 9
19/04/10 07:51:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 8 from BlockManagerMaster.
19/04/10 07:51:45 INFO BlockManagerMaster: Removal of executor 8 requested
19/04/10 07:51:45 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asked to remove non-existent executor 8
19/04/10 07:51:46 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.69.197:56016) with ID 20
19/04/10 07:51:46 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.94.21:53500) with ID 11
19/04/10 07:51:46 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.69.197:37693 with 2.1 GB RAM, BlockManagerId(20, 10.233.69.197, 37693, None)
19/04/10 07:51:47 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.94.21:36235 with 2.1 GB RAM, BlockManagerId(11, 10.233.94.21, 36235, None)
19/04/10 07:51:47 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.76.125:40980) with ID 17
19/04/10 07:51:47 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.76.125:38937 with 2.1 GB RAM, BlockManagerId(17, 10.233.76.125, 38937, None)
19/04/10 07:51:48 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.116.231:42718) with ID 18
19/04/10 07:51:48 INFO BlockManagerMasterEndpoint: Registering block manager 10.233.116.231:46783 with 2.1 GB RAM, BlockManagerId(18, 10.233.116.231, 46783, None)
19/04/10 07:51:48 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.233.126.196:57320) with ID 16
19/04/10 07:51:48 INFO ExecutorPodsAllocator: Going to request 3 executors from Kubernetes.
19/04/10 07:51:48 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
Validation loss at epoch 1: 0.9997375889277974
Validation loss at epoch 2: 0.9994678310532596
Validation loss at epoch 3: 0.9991953987643077
Validation loss at epoch 4: 0.9989408077983599
Validation loss at epoch 5: 0.9986836337084375
Validation loss at epoch 6: 0.9984168384720116
Validation loss at epoch 7: 0.9981536979769445
Validation loss at epoch 8: 0.9978889717274461
Validation loss at epoch 9: 0.9976233342987076
Validation loss at epoch 10: 0.9973625362094772
Validation loss at epoch 11: 0.9970950953336174
Validation loss at epoch 12: 0.9968400883790187
Validation loss at epoch 13: 0.9965711161207841
Validation loss at epoch 14: 0.9962983341862109
Validation loss at epoch 15: 0.9960329312544501
Validation loss at epoch 16: 0.9957697779864815
Validation loss at epoch 17: 0.9955123288920434
Validation loss at epoch 18: 0.9952458436830017
Validation loss at epoch 19: 0.9949829780303533
Validation loss at epoch 20: 0.9947155056849053
Validation loss at epoch 21: 0.9944444979727832
Validation loss at epoch 22: 0.9941815771109398
Validation loss at epoch 23: 0.9939222907859129
Validation loss at epoch 24: 0.9936516392674589
Validation loss at epoch 25: 0.9933813099061071
Validation loss at epoch 26: 0.9931184539647986
Validation loss at epoch 27: 0.9928539308920162
Validation loss at epoch 28: 0.9925860817316233
Validation loss at epoch 29: 0.9923224876443744
Validation loss at epoch 30: 0.9920578850633078
Validation loss at epoch 31: 0.9917979840733794
Validation loss at epoch 32: 0.9915299704420409
Validation loss at epoch 33: 0.9912554584308849
Validation loss at epoch 34: 0.9909928051311527
Validation loss at epoch 35: 0.9907348857165686
Validation loss at epoch 36: 0.9904711561743503
Validation loss at epoch 37: 0.9902002392015957
Validation loss at epoch 38: 0.9899336095462954
Validation loss at epoch 39: 0.9896695842489212
Validation loss at epoch 40: 0.9894018045731731
computing logs files ...
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 640.0 failed 4 times, most recent failure: Lost task 3.3 in stage 640.0 (TID 6458, 10.233.78.32, executor 1): java.io.IOException: Failed to connect to spark-1554882676220-driver-svc.cs449g5.svc:7078
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:693)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:496)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:806)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:798)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:798)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:370)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.lang.NullPointerException
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1402)
		... 17 more
Caused by: java.net.UnknownHostException: spark-1554882676220-driver-svc.cs449g5.svc
	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at java.net.InetAddress.getByName(InetAddress.java:1077)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:146)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:143)
	at java.security.AccessController.doPrivileged(Native Method)
	at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:143)
	at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:43)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:63)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:55)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:57)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:32)
	at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:108)
	at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)
	at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:49)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:188)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:174)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)
	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
	at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:978)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:512)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	... 1 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)
	at com.github.yannbolliger.g5.parallel.sgd.spark.SVM.accuracy(SVM.scala:90)
	at com.github.yannbolliger.g5.parallel.sgd.spark.ParallelSGDApp$.delayedEndpoint$com$github$yannbolliger$g5$parallel$sgd$spark$ParallelSGDApp$1(ParallelSGDApp.scala:56)
	at com.github.yannbolliger.g5.parallel.sgd.spark.ParallelSGDApp$delayedInit$body.apply(ParallelSGDApp.scala:6)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.App$class.main(App.scala:76)
	at com.github.yannbolliger.g5.parallel.sgd.spark.ParallelSGDApp$.main(ParallelSGDApp.scala:6)
	at com.github.yannbolliger.g5.parallel.sgd.spark.ParallelSGDApp.main(ParallelSGDApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Failed to connect to spark-1554882676220-driver-svc.cs449g5.svc:7078
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:693)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:496)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:806)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:798)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:798)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:370)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.lang.NullPointerException
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1402)
		... 17 more
Caused by: java.net.UnknownHostException: spark-1554882676220-driver-svc.cs449g5.svc
	at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
	at java.net.InetAddress.getByName(InetAddress.java:1077)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:146)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:143)
	at java.security.AccessController.doPrivileged(Native Method)
	at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:143)
	at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:43)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:63)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:55)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:57)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:32)
	at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:108)
	at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)
	at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:49)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:188)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:174)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)
	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
	at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:978)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:512)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	... 1 more
